## 빅데이터

### Resource Management

#### 기존 하둡의 문제

확장성과 안정성이 높다보니 누구나 사용했다. 그런데 본래 목적과 다르게 map만을 사용하는 어플리케이션을 만들어서 웹 서버에서 이용했다. (재시작이 잘 되는 하둡 특성을 이용.) 또 머신러닝에서 literative한 작업만을 수행하게 했다. 

#### Multi tenacy

여러 하둡 클라서터를 함께 사용한다. 머신러닝 알고리즘이나 커뮤니케이션 패턴 등이 필요했다. 

#### Hadoop on demand

이러한 문제를 해결하기 위해 haddop on demand (스파크로 봐도 된다.) 여러 사람이 쓰고싶은 것이다. 굉장히 큰 서버 풀에 대해 일정 부분 떼어서 주는것. 

이것이 실패했다. 그 이유는 기본적으로 스파크와 하둡 모두 데이터 통신을 최소로 하는것이 목표이다. 그런데 이런식으로 사용하면 locality가 떨어진다. 또한 항상 static하게 리소스를 설정하다보니 최대치로 설정하게 되었고 이것이 비효율적이게 됌. 

#### shared cluster instead of ondemand

하나의 큰 클러스터를 공유해서 사용하는것. 마스터 노드가 굉장히 많은 문제를 처리해야 하는 문제 발생. 사람들이 각각 버전을 다르게 이용하고 싶은데 하나이다보니 이것이 불가능

#### YARN

Yet Another resource negotiator

스파크나 하둡이나 다른 DB 딥러닝 등 하나의 어플리케이션으로 보이게 됌. 기존의 하둡이나 스파크의 자원 공유 단점 극복. 전통 하둡에서는 마스터가 있고 워커가 있어서 일을 처리하게 했는데, 얀은 리소스 매니저 노드 매니저, 어플리케이션 마스터를 두었다. 

##### 리소스매니저

전체 클러스터 중 하나밖에 존재하지 않음. 전체 클러스터의 자원 모니터링. 여러 다양한 어플리케이션들을 공정하게 자원 활용할 수 있도록 함. 특정 어플리케이션의 요구사항들을 들어주고 할당. 

사용자는 본인이 동작시키고자 하는 작업을 리소스매니저에게 submit. 리소스 매니저가 노드 매니저에게 물어보고 어플리케이션 매니저에게 가고.. 

다이나믹하게 실시간으로 컨테이너가 실행에 따라 자원을 더주고 안주고.,

##### 어플리케이션 마스터

리소스 매니저가 전체 클러스터 상태를 관리한다고 하면 어플리케이션 마스터는 리소스매니저로 받은 자원을 할당하여 각 응용프로그램을 관리한다고 생각하면 된다. 각 응용프로그램을 어떻게 구성할지 어플리케이션 마스터가 알고 결정한다. 

리소스 매니저가 자원을 할당하면 어플리케이션 마스터가 응용에 맞게 잘 만든다. 

##### 노드 매니저

서버가 만일 100대라면 각 서버에 호스트당 하나 동작하는 프로세스. 각 서버에 얼마만큼 자원이 할당되고 활용되는지 모니터링. 

- container launch context : 환경변수 실행파일 실행 변수 등을 가지고 있는것.

지속적으로 리소스 매니저에게 heartbeat날려서 상태 전송

#### 얀 프레임워크, 어플리케이션 작성자

사용자는 CLC(containter launch context) 라는 어플리케이션을 어떻게 실행해야 하는지 적어놓은 것을 리소스 매니저에게 보내면 리소스 매니저는 어플리케이션 매니저를 실행한다. 이 CLC를 노드 매니저에게 보내주고 노드 매니저는 실행. 자겅ㅂ 끝나고 리소스 매니저에게 반납. 

#### Fault tolerance and availability

리소스 매니저

- 모든 상태를 가짐. 모니터링 하다가 잘 동작하지 않으면 다시 실행. 안정성을 리소스 매니저가 화곱

노드 매니저 

- 하트비트를 계속 보낸다.
- 컨테이너를 죽이고 어플리케이션 마스터에게 report.

어플리케이션 마스터 

- 프로세스라 죽으면 어차피 리소스 매니저가 다시 실행. 